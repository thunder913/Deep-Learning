{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do RNN and LSTM have Long Memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li, Guangjian Tian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Published in: 37th International Conference on Machine Learning, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper investigates whether Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks truly have long memory—the ability to retain information for a long time. Even though LSTMs were designed to overcome the short memory issue in RNNs, the authors show that both RNNs and LSTMs do not have long memory from a statistical perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem\n",
    "\n",
    "In machine learning we usually work with sequential data and the models like RNNs struggle with remembering long-term patterns, due to the vanishing gradient problem - old information fades away as the sequence gets longer and longer. Working with deep learning algorithms usually means working with very big datasets and for the model to forget the first passed data is a big issue. LSTMs are designed to fix this problem, but the researchers in the article are going to question, how truly long term memory does it have.\n",
    " \n",
    " The article mentions some related past research, that has been conducted on this topic. First it mentions that traditional statistical models can handle long memory well, but are not flexible like neural networks. There are also some researches on LSTM's long term memory, but they are very little count. There have been some alternatives to RNN, that try to keep long term memory. The model from this article will be compared with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory property of recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is long memory?\n",
    "\n",
    "First long memory is defined from a statistical perspective.\n",
    "\n",
    "\n",
    "The autocovariance function measures how strongly past values influence future values in a time series.\n",
    "\n",
    "$$\\sum_{k=-\\infty}^{\\infty } \\gamma x (k) = \\infty \\text{ (long memory)} $$\n",
    "\n",
    "$$0 < \\sum_{k=-\\infty}^{\\infty } \\gamma x (k) < \\infty \\text{ (short memory)} $$\n",
    "\n",
    "$$where:\n",
    "\\\\ X_t \\text{ is the time series}\n",
    "\\\\ \\text{k represents the time lag}\n",
    "\\\\ y_x(k) \\text{is the autocovariance function, which measures how much past values influence the present}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the sum of the covariances is infinite, the time series has long memory. If the sum is finite, the time series has short memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do the models have short memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, there is an explanation on how RNN's and LSTM's work and process sequences mathematically. A general recurrent network consists of  inputs (x(t)), hidden states (s(t)) and outputs (y(t)). The model assumes the target sequence as:\n",
    "\n",
    "$$ y(t)= z(t) + s(t) $$\n",
    "\n",
    "The network is basically trying to predict y(t) using its internal hidden states.\n",
    "\n",
    "To analyze the memory of recurrent networks, they are modeles as a Markov Process, which means that the future state depends only on the previous state, not the full history. If an RNN follows this structure, it cannot retain long memory, because old information is lost over time.\n",
    "\n",
    "Using hidden states allows us to pass along some memory, but they also follow a Markov structure.\n",
    "\n",
    "In RNN's the hidden state depends only on s(t-1), like in the Markov chains and that makes older information gradually fade away.\n",
    "\n",
    "LSTM modify the basic RNN structure by adding gates to control memory updates and that way LSTMs can retain information lonjger than RNNs, but their forget gate often remove old information too aggressive and this limits long memory retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up mathematical proofs are shown, that RNN and LSTM models only have short memory, since both models have update rules, that shrink past data at an exponential rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New way to define long memory in neural networks\n",
    "\n",
    "A new way to define long memory in neural networks is proposed.\n",
    "$$\n",
    "y(t) = \\sum_{k=0}^{\\infty} A_k x(t-k) + \\epsilon(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y(t) is the output at time \n",
    "\n",
    "x(t−k) is the input from \n",
    "\n",
    "k steps in the past.\n",
    "\n",
    "​Ak  is a weight function that controls how much past data influences the present.\n",
    "\n",
    "ϵ(t) is a noise term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea with tis memory layer is  to make Ak decrease quickly, in order to achieve long memory and make past inputs have strong influence.\n",
    "\n",
    "For this we need k to be as high of a value as possible.\n",
    "\n",
    "$$ \n",
    "A_k \\sim k^{-d-1} \\quad \\text{as } k \\to \\infty, \\text{ where } d \\in (0, 0.5)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the parameter d controls the strength of long memory and we need a small d, to make the memory last longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Long Memory Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory-augmented RNN (MRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(x(t); d) = \\left( (I - B)^d - I \\right) x(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the memory filter, that the authors introduces for RNN. B is the backshift operator and d is the memory parameter. So the long memory hidden unit is introduced in the hidden state like following:\n",
    "\n",
    "$$\n",
    "m(t) = \\tanh(W_m m(t-1) + W_x x(t) + b_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rnn_structure.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the structure, the m(t) is used in parallel with the hidden filter h(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z(t) = g(W_{zh} h(t) + W_{zm} m(t) + b_z) + \\epsilon(t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory parameter d(t) controls how much past information is kept and it is dynamically calculated based on previous memory and hidden states. The use of sigmoid function ensures d(t) is kept in the rage (0, 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ d(t) = \\frac{1}{2} \\sigma\\left(W_d [d(t-1), h(t-1), m(t-1), x(t)] + b_d \\right)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Memory-augmented LSTM (MLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal LSTM has its cell state like this:\n",
    "\n",
    "$$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that if ft is small, past informations fades too fast.\n",
    "\n",
    "So MLSTM replaces the equation with this:\n",
    "\n",
    "$$ (I - B)_d \\, c_t = i_t \\odot \\tilde{c}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of simply using forget gates, it smoothly blends past cell states using a fractional memory filter. This gradually forgets older ijnformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory parameter is calculated dynamically:\n",
    "\n",
    "$$ d_t = \\frac{2}{1} \\sigma\\left(W_d [d_{t-1}, h_{t-1}, x_t] + b_d \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is applied in the cell state like this:\n",
    "\n",
    "$$c_t = - \\sum_{j=1}^{K} w_j (d_t) c_{t-j} + i_t \\odot \\tilde{c}_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the hidden state is computed using this formula:\n",
    "\n",
    "$$ h_t = o_t \\odot \\tanh(c_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors have provided the actual code for free from their github page. I have put it in the folder mrnn_mlstm_experiment. To run it we just follow the instructions in the readme file. Basically we run the following command:\n",
    "\n",
    "python .\\train.py --dataset '{dataset}' --algorithm '{the algorithm}' --epochs 50\n",
    "\n",
    "I have chosen to run it on 'tree7' as dataset and on DJI (Dow Jones Index) and those are the results for MLSTM:\n",
    "\n",
    "**MLSTM** results:\n",
    "\n",
    "tree7:\n",
    "RMSE:[0.2990356773379933]\n",
    "MAE:[0.23424398632834936]\n",
    "\n",
    "DJI:\n",
    "RMSE:[0.0027341451016919425]\n",
    "MAE:[0.0021653839783128855]\n",
    "\n",
    "\n",
    "**LSTM** results for comparison:\n",
    "\n",
    "tree7\n",
    "RMSE:[0.3039184161273762]\n",
    "MAE:[0.2365812977560829]\n",
    "\n",
    "DJI:\n",
    "RMSE:[0.0027319818988946667]\n",
    "MAE:[0.0021302994670985313]\n",
    "\n",
    "The training time for the MLSTM was 20s per epoch, while the normal LSTM is 0.8s, so I wouldn't really recommend using this model if you are having very big datasets, which is usually the case for neural networks.\n",
    "\n",
    "As in the article, even the results of the MLSTM model are not that great, compared with the normal LSTM. It surely is a big better with the Long-term memory theoretically than the LSTM, but the training time and performance don't really make it a viable option in my opinion.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
