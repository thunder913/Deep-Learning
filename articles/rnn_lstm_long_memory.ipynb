{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do RNN and LSTM have Long Memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li, Guangjian Tian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Published in: 37th International Conference on Machine Learning, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper investigates whether Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks truly have long memory—the ability to retain information for a long time. Even though LSTMs were designed to overcome the short memory issue in RNNs, the authors show that both RNNs and LSTMs do not have long memory from a statistical perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem\n",
    "\n",
    "In machine learning we usually work with sequential data and the models like RNNs struggle with remembering long-term patterns, due to the vanishing gradient problem - old information fades away as the sequence gets longer and longer. Working with deep learning algorithms usually means working with very big datasets and for the model to forget the first passed data is a big issue. LSTMs are designed to fix this problem, but the researchers in the article are going to question, how truly long term memory does it have.\n",
    " \n",
    " The article mentions some related past research, that has been conducted on this topic. First it mentions that traditional statistical models can handle long memory well, but are not flexible like neural networks. There are also some researches on LSTM's long term memory, but they are very little count. There have been some alternatives to RNN, that try to keep long term memory. The model from this article will be compared with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory property of recurrent networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is long memory?\n",
    "\n",
    "First long memory is defined from a statistical perspective.\n",
    "\n",
    "\n",
    "The autocovariance function measures how strongly past values influence future values in a time series.\n",
    "\n",
    "$$\\sum_{k=-\\infty}^{\\infty } \\gamma x (k) = \\infty \\text{ (long memory)} $$\n",
    "\n",
    "$$0 < \\sum_{k=-\\infty}^{\\infty } \\gamma x (k) < \\infty \\text{ (short memory)} $$\n",
    "\n",
    "$$where:\n",
    "\\\\ X_t \\text{ is the time series}\n",
    "\\\\ \\text{k represents the time lag}\n",
    "\\\\ y_x(k) \\text{is the autocovariance function, which measures how much past values influence the present}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the sum of the covariances is infinite, the time series has long memory. If the sum is finite, the time series has short memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do the models have short memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, there is an explanation on how RNN's and LSTM's work and process sequences mathematically. A general recurrent network consists of  inputs (x(t)), hidden states (s(t)) and outputs (y(t)). The model assumes the target sequence as:\n",
    "\n",
    "$$ y(t)= z(t) + s(t) $$\n",
    "\n",
    "The network is basically trying to predict y(t) using its internal hidden states.\n",
    "\n",
    "To analyze the memory of recurrent networks, they are modeles as a Markov Process, which means that the future state depends only on the previous state, not the full history. If an RNN follows this structure, it cannot retain long memory, because old information is lost over time.\n",
    "\n",
    "Using hidden states allows us to pass along some memory, but they also follow a Markov structure.\n",
    "\n",
    "In RNN's the hidden state depends only on s(t-1), like in the Markov chains and that makes older information gradually fade away.\n",
    "\n",
    "LSTM modify the basic RNN structure by adding gates to control memory updates and that way LSTMs can retain information lonjger than RNNs, but their forget gate often remove old information too aggressive and this limits long memory retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up mathematical proofs are shown, that RNN and LSTM models only have short memory, since both models have update rules, that shrink past data at an exponential rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New way to define long memory in neural networks\n",
    "\n",
    "A new way to define long memory in neural networks is proposed.\n",
    "$$\n",
    "y(t) = \\sum_{k=0}^{\\infty} A_k x(t-k) + \\epsilon(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y(t) is the output at time \n",
    "\n",
    "x(t−k) is the input from \n",
    "\n",
    "k steps in the past.\n",
    "\n",
    "​Ak  is a weight function that controls how much past data influences the present.\n",
    "\n",
    "ϵ(t) is a noise term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea with tis memory layer is  to make Ak decrease quickly, in order to achieve long memory and make past inputs have strong influence.\n",
    "\n",
    "For this we need k to be as high of a value as possible.\n",
    "\n",
    "$$ \n",
    "A_k \\sim k^{-d-1} \\quad \\text{as } k \\to \\infty, \\text{ where } d \\in (0, 0.5)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the parameter d controls the strength of long memory and we need a small d, to make the memory last longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Long Memory Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory-augmented RNN (MRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(x(t); d) = \\left( (I - B)^d - I \\right) x(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the memory filter, that the authors introduces for RNN. B is the backshift operator and d is the memory parameter. So the long memory hidden unit is introduced in the hidden state like following:\n",
    "\n",
    "$$\n",
    "m(t) = \\tanh(W_m m(t-1) + W_x x(t) + b_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rnn_structure.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the structure, the m(t) is used in parallel with the hidden filter h(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z(t) = g(W_{zh} h(t) + W_{zm} m(t) + b_z) + \\epsilon(t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory parameter d(t) controls how much past information is kept and it is dynamically calculated based on previous memory and hidden states. The use of sigmoid function ensures d(t) is kept in the rage (0, 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ d(t) = \\frac{1}{2} \\sigma\\left(W_d [d(t-1), h(t-1), m(t-1), x(t)] + b_d \\right)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Memory-augmented LSTM (MLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal LSTM has its cell state like this:\n",
    "\n",
    "$$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that if ft is small, past informations fades too fast.\n",
    "\n",
    "So MLSTM replaces the equation with this:\n",
    "\n",
    "$$ (I - B)_d \\, c_t = i_t \\odot \\tilde{c}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of simply using forget gates, it smoothly blends past cell states using a fractional memory filter. This gradually forgets older ijnformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory paramter is calculated dynamically:\n",
    "\n",
    "$$ d_t = \\frac{2}{1} \\sigma\\left(W_d [d_{t-1}, h_{t-1}, x_t] + b_d \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is applied in the cell state like this:\n",
    "\n",
    "$$c_t = - \\sum_{j=1}^{K} w_j (d_t) c_{t-j} + i_t \\odot \\tilde{c}_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the hidden state is computed using this formula:\n",
    "\n",
    "$$ h_t = o_t \\odot \\tanh(c_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO explain it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1131487683.py, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 66\u001b[1;36m\u001b[0m\n\u001b[1;33m    model = Model(inputs, outputs)a\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, LSTM, Layer, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fetch_dji_data():\n",
    "    dji = yf.download('^DJI', start='2000-01-01', end='2023-12-31')\n",
    "    if 'Close' in dji.columns:\n",
    "        returns = dji['Close'].pct_change().dropna()\n",
    "    else:\n",
    "        raise KeyError(\"Column 'Close' not found in the dataset.\")\n",
    "    return returns.values\n",
    "\n",
    "def prepare_data(series, seq_len=20):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - seq_len):\n",
    "        X.append(series[i:i+seq_len])\n",
    "        y.append(series[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_rnn(seq_len):\n",
    "    inputs = Input(shape=(seq_len, 1))\n",
    "    x = SimpleRNN(50, activation='tanh', return_sequences=False)(inputs)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_lstm(seq_len):\n",
    "    inputs = Input(shape=(seq_len, 1))\n",
    "    x = LSTM(50, activation='tanh', return_sequences=False)(inputs)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Custom Memory Filter Layer\n",
    "class MemoryFilter(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(MemoryFilter, self).__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.d = self.add_weight(shape=(1,), initializer='uniform', trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        memory_weights = [tf.math.exp(-self.d * i) for i in range(self.seq_len)]\n",
    "        memory_weights = tf.expand_dims(memory_weights, axis=0)\n",
    "        return tf.reduce_sum(inputs * memory_weights, axis=1, keepdims=True)\n",
    "\n",
    "def build_mrnn(seq_len):\n",
    "    inputs = Input(shape=(seq_len, 1))\n",
    "    memory = MemoryFilter(seq_len)(inputs)\n",
    "    x = SimpleRNN(50, activation='tanh')(memory)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_mlstm(seq_len):\n",
    "    inputs = Input(shape=(seq_len, 1))\n",
    "    memory = MemoryFilter(seq_len)(inputs)\n",
    "    x = LSTM(50, activation='tanh')(memory)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)a\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "seq_len = 20\n",
    "series = fetch_dji_data()\n",
    "X, y = prepare_data(series, seq_len)\n",
    "X = X.reshape(-1, seq_len, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "models = {\n",
    "    \"RNN\": build_rnn(seq_len),\n",
    "    \"LSTM\": build_lstm(seq_len),\n",
    "    \"MRNN\": build_mrnn(seq_len),\n",
    "    \"MLSTM\": build_mlstm(seq_len)\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    histories[name] = model.fit(X, y, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, history in histories.items():\n",
    "    plt.plot(history.history['loss'], label=f'{name} Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss Comparison on DJI Data\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
